{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import operator\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_data_within_brackets(test_str):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    skip3c = 0\n",
    "    for i in test_str:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == '<':\n",
    "            skip3c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif i == '>' and skip3c > 0:\n",
    "            skip3c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0 and skip3c == 0:\n",
    "            ret += i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPOSTag (sentence,word):\n",
    "    PosSent = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    for word_Tag in PosSent:\n",
    "        if(word_Tag[0].lower()==word.lower()):\n",
    "            return (word.lower() + \"_\"+word_Tag[1])\n",
    "    return (\"word not in sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load MT file\n",
    "inputFile = open('F:\\\\ParaPhrase\\\\LanguageData\\\\NB_EN_1M_LessEdited_a_Mod.txt',  'rb')\n",
    "data = inputFile.readlines()\n",
    "inputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is therefore possible that some of the interests may be excluded at the outset.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utterances = []\n",
    "data[1].decode(\"utf-8\").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AddLines(Utterances,data):\n",
    "    for lines in data:\n",
    "        lines = lines.decode(\"utf-8\").strip()\n",
    "        temp = sent_tokenize(lines)\n",
    "        for line in temp:\n",
    "            line = line.replace(\",\",\" \")\n",
    "            line = line.replace (\"(\",\" \")\n",
    "            line = line.replace (\")\",\" \")\n",
    "            line = line.replace (\"*\",\" \")\n",
    "            line = line.replace (\"#\",\" \")\n",
    "            line = line.replace (\".\",\" . \")\n",
    "            line = line.replace (\"?\",\" ? \")\n",
    "            line = line.replace (\"!\",\" ! \")\n",
    "            line = line.replace (\" 's\",\"'s\")\n",
    "            line = line.replace (\" 'd\",\"'d\")\n",
    "            line = line.replace (\"â\",\" \")\n",
    "            line = line.replace (\"/\",\" \")\n",
    "            line = ''.join([i for i in line if not i.isdigit()])\n",
    "            Utterances.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AddLines(Utterances,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(Utterances))\n",
    "print(Utterances[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFile = open('F:\\\\ParaPhrase\\\\LanguageData\\\\NB_EN_1M_LessEdited_d_Mod.txt',  'rb')\n",
    "data = inputFile.readlines()\n",
    "inputFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AddLines(Utterances,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(Utterances))\n",
    "print(Utterances[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NewUtterancesUnique = list(set(Utterances))\n",
    "print(len(NewUtterancesUnique))\n",
    "print((NewUtterancesUnique)[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputFile = open('LanguageData.pkl', 'wb')\n",
    "pickle.dump(Utterances,outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile = open('LanguageData_NewUtterancesUnique.pkl', 'wb')\n",
    "pickle.dump(NewUtterancesUnique,outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(NewUtterancesUnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count =1\n",
    "totalInstances = len(NewUtterancesUnique)\n",
    "for i in range(0,totalInstances,25000):\n",
    "    filename = \"LanguageDataUtterances\"+str(count)+\".txt\"\n",
    "    print(filename)\n",
    "    print(i)\n",
    "    count =count+1\n",
    "    my_file = open(filename, \"wb\")\n",
    "    for utt in NewUtterancesUnique[i+1:min(i+25000,totalInstances)]:\n",
    "        my_file.write(str(utt).encode(\"utf-8\"))\n",
    "    my_file.close()\n",
    "    #print(str(i+50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"F:\\\\ParaPhrase\\\\conllFiles2\"\n",
    "FilesList = [os.path.join(dp, f) for dp, dn, filenames in os.walk(PATH) for f in filenames if os.path.splitext(f)[1] == '.conll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConllLine:\n",
    "    def __init__(self, index, targetWord,tag,affectIndex,relation):\n",
    "        self.index = index\n",
    "        self.targetWord =targetWord\n",
    "        self.tag=tag\n",
    "        self.affectIndex=affectIndex\n",
    "        self.relation=relation\n",
    "        self.FinalString=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances1.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances10.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances11.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances12.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances13.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances14.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances15.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances16.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances17.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances18.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances19.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances2.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances20.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances21.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances22.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances23.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances24.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances25.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances26.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances27.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances28.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances29.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances3.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances30.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances31.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances32.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances33.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances34.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances35.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances36.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances37.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances38.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances39.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances4.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances40.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances41.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances42.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances43.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances44.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances45.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances46.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances47.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances48.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances49.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances5.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances50.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances51.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances52.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances53.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances54.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances55.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances56.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances57.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances58.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances59.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances6.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances60.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances61.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances62.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances63.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances64.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances65.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances66.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances67.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances68.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances69.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances7.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances70.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances71.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances72.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances73.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances74.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances75.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances76.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances77.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances78.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances79.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances8.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances80.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances81.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances82.txt.conll\n",
      "F:\\ParaPhrase\\conllFiles2\\LanguageDataUtterances9.txt.conll\n"
     ]
    }
   ],
   "source": [
    "ConllList=[]\n",
    "count = 1\n",
    "lineCount = 1\n",
    "for fname in FilesList:\n",
    "    print(fname)\n",
    "    with open(fname) as f:\n",
    "        contents = f.readlines()\n",
    "        ConnlUnit=[]\n",
    "        for line in contents:\n",
    "            lineCount=lineCount+1\n",
    "            words = line.split(\"\\t\")\n",
    "            if (len(words)== 7):\n",
    "                index=words[0]\n",
    "                targetWord=words[1]\n",
    "                tag = words[3]\n",
    "                affectIndex = words[5]\n",
    "                relation = words[6]\n",
    "                ConnlUnit.append(ConllLine(index,targetWord,tag,affectIndex,relation))\n",
    "            else:\n",
    "                ConllList.append(ConnlUnit)\n",
    "                ConnlUnit=[]\n",
    "                count = count +1\n",
    "        ConllList.append(ConnlUnit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2167936\n",
      "2167855\n",
      "50120345\n"
     ]
    }
   ],
   "source": [
    "print(len(ConllList))\n",
    "print(count)\n",
    "print(lineCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lineCount1 = 1\n",
    "sentences = []\n",
    "for cU in ConllList:\n",
    "    ConnlUnit=[]\n",
    "    for cl in cU:\n",
    "        lineCount1=lineCount1+1\n",
    "        AI = int(cl.affectIndex) - 1\n",
    "        if(AI>=0):\n",
    "            cl.FinalString = cl.targetWord + \"_\"+cl.tag + \" \"+ cl.relation.strip() + \" \"+ cU[AI].targetWord+\"_\"+cU[AI].tag\n",
    "        else :\n",
    "            cl.FinalString = cl.targetWord + \"_\"+cl.tag + \" \"+ cl.relation.strip()\n",
    "        sentences.append(cl.FinalString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5e87ca5b4293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizedSentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtokenizedSentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\gilopez\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py_3.4\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[0;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gilopez\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py_3.4\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[1;32m--> 107\u001b[1;33m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gilopez\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py_3.4\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPARENS_BRACKETS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizedSentences = []\n",
    "for sent in sentences:\n",
    "    tokenizedSentences.append(nltk.word_tokenize (sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['willingness_NN ROOT',\n",
       " 'to_TO mark take_VB',\n",
       " 'take_VB acl willingness_NN',\n",
       " 'responsibility_NN dobj take_VB',\n",
       " 'and_CC cc take_VB',\n",
       " 'make_VB conj take_VB',\n",
       " 'amends_NNS nsubj strengthen_VB',\n",
       " 'can_MD aux strengthen_VB',\n",
       " 'strengthen_VB ccomp make_VB',\n",
       " 'the_DT det bond_NN',\n",
       " 'attachment_NN compound bond_NN',\n",
       " 'bond_NN dobj strengthen_VB',\n",
       " '._. punct willingness_NN',\n",
       " 'The_DT det one_CD',\n",
       " 'one_CD nsubjpass notified_VBN',\n",
       " 'recently_RB advmod discovered_VBN',\n",
       " 'discovered_VBN acl one_CD',\n",
       " 'in_IN case countries_NNS',\n",
       " 'Northern_JJ amod countries_NNS',\n",
       " 'European_JJ amod countries_NNS',\n",
       " 'countries_NNS nmod discovered_VBN',\n",
       " 'in_IN case August_NNP',\n",
       " 'August_NNP nmod discovered_VBN',\n",
       " 'was_VBD auxpass notified_VBN',\n",
       " 'Serotype_NNP nsubjpass notified_VBN',\n",
       " 'previously_RB advmod notified_VBN',\n",
       " 'notified_VBN ROOT',\n",
       " 'in_IN case Africa_NNP',\n",
       " 'South_NNP compound Africa_NNP',\n",
       " 'Africa_NNP nmod notified_VBN',\n",
       " 'and_CC cc Africa_NNP',\n",
       " 'India_NNP conj Africa_NNP',\n",
       " '._. punct notified_VBN',\n",
       " 'As_IN case group_NN',\n",
       " 'the_DT det world_NN',\n",
       " 'world_NN nmod:poss group_NN',\n",
       " \"'s_POS case world_NN\",\n",
       " 'largest_JJS amod group_NN',\n",
       " 'exchange_NN compound group_NN',\n",
       " 'group_NN nmod home_NN',\n",
       " 'by_IN case number_NN',\n",
       " 'number_NN nmod group_NN',\n",
       " 'of_IN case listings_NNS',\n",
       " 'listings_NNS nmod number_NN',\n",
       " 'and_CC cc listings_NNS',\n",
       " 'market_NN compound Euronext_NNP',\n",
       " 'capitalization_NN compound Euronext_NNP',\n",
       " 'NYSE_NNP compound Euronext_NNP',\n",
       " 'Euronext_NNP conj listings_NNS',\n",
       " 'is_VBZ cop home_NN',\n",
       " 'home_NN ROOT',\n",
       " 'to_TO case companies_NNS',\n",
       " 'over_IN case companies_NNS',\n",
       " 'listed_VBN amod companies_NNS',\n",
       " 'companies_NNS nmod home_NN',\n",
       " 'representing_VBG acl home_NN',\n",
       " 'a_DT det $_$',\n",
       " 'combined_JJ amod $_$',\n",
       " '$_$ dobj representing_VBG',\n",
       " '._. punct home_NN',\n",
       " '._. ROOT',\n",
       " '‚_NN ROOT',\n",
       " '¬_CD compound trillion_CD',\n",
       " 'trillion_CD nummod ‚_NN',\n",
       " 'as_IN case June_NNP',\n",
       " 'of_IN case June_NNP',\n",
       " 'June_NNP nmod ‚_NN',\n",
       " 'in_IN case capitalization_NN',\n",
       " 'total_JJ amod capitalization_NN',\n",
       " 'global_JJ amod capitalization_NN',\n",
       " 'market_NN compound capitalization_NN',\n",
       " 'capitalization_NN nmod ‚_NN',\n",
       " 'more_JJR advmod four_CD',\n",
       " 'than_IN mwe more_JJR',\n",
       " 'four_CD nummod times_NNS',\n",
       " 'times_NNS nmod:tmod capitalization_NN',\n",
       " 'that_IN acl times_NNS',\n",
       " 'of_IN case group_NN',\n",
       " 'any_DT det group_NN',\n",
       " 'other_JJ amod group_NN',\n",
       " 'exchange_NN compound group_NN',\n",
       " 'group_NN nmod that_IN',\n",
       " '._. punct ‚_NN',\n",
       " 'ChartDirector_NNP nmod:poss engine_NN',\n",
       " \"'s_POS case ChartDirector_NNP\",\n",
       " 'built-in_JJ amod engine_NN',\n",
       " 'multi-threaded_JJ amod engine_NN',\n",
       " 'graphics_NNS compound engine_NN',\n",
       " 'engine_NN nsubjpass designed_VBN',\n",
       " 'is_VBZ auxpass designed_VBN',\n",
       " 'specially_RB advmod designed_VBN',\n",
       " 'designed_VBN ROOT',\n",
       " 'for_IN case graphics_NNS',\n",
       " 'high_JJ amod graphics_NNS',\n",
       " 'performance_NN compound graphics_NNS',\n",
       " 'server_NN compound graphics_NNS',\n",
       " 'side_NN compound graphics_NNS',\n",
       " 'graphics_NNS nmod designed_VBN',\n",
       " '._. punct designed_VBN']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str1 = \"this is string example.wow!!!\";\n",
    "str2 = \".\";\n",
    "str1.find(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train word2vec on the sentences (https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "model = gensim.models.Word2Vec(tokenizedSentences, min_count=20,window=100,size=300,sg=1,iter =10)\n",
    "fname = \"LanguageData_deppos_skip_iter10_size_300_min_count_20\"\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \n",
    "vocab = model.index2word\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"denver_NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"will_MD\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"today_NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"than_IN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"guess_VBP\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"day_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"not_RB\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = model.index2word\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#increased size to 500 and min count is 20\n",
    "model1 = gensim.models.Word2Vec(tokenizedSentences, min_count=20,window=100,size=500,sg=1,iter =10)\n",
    "fname = \"LanguageData_deppos_minCount20_skip_iter10_size_500\"\n",
    "model1.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = model1.index2word\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"LanguageData_deppos_minCount20_skip_iter10_size_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"F:\\\\ParaPhrase\\\\Notebooks\\\\UnigramContext.txt\"\n",
    "unigrams = []\n",
    "with open(path,'r') as source:\n",
    "    for line in source:\n",
    "        fields = line.split('\\t')\n",
    "        unigrams.append(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option_NN\n",
      "[('options_NNS', 0.5430517196655273), ('infoservice_NN', 0.4274654686450958), ('movix_NN', 0.4150727689266205), ('pxelinux_NN', 0.41237908601760864), ('reconnect_NN', 0.4057256579399109), ('subcommand_NN', 0.4039083421230316), ('previewer_NN', 0.40203583240509033), ('ldifde_NN', 0.3999919891357422), ('keyfile_NN', 0.3991797864437103), ('auto-processing_NN', 0.3977552354335785), ('overwrite_NN', 0.396797239780426), ('wsb_NN', 0.39648187160491943), ('autofilter_NN', 0.3960217237472534), ('dwm_NN', 0.3953208327293396), ('feature_NN', 0.3940016031265259)]\n"
     ]
    }
   ],
   "source": [
    "tagged_word = getPOSTag (\"is that an  option\",\"option\")\n",
    "print(tagged_word)\n",
    "result = model.most_similar(tagged_word,topn=15)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open( \"UnigramCandidates.tsv\", \"w\" ) as f :\n",
    "    for uni in unigrams:\n",
    "        tagged_word = getPOSTag (uni[0],uni[1].strip())\n",
    "        tagged_word_pos = tagged_word.split('_')\n",
    "\n",
    "        f.write ( \"\\\" \" + uni[1].strip().upper() + \"\\\" \"+ \" in context of  \"+ \"\\\" \" + uni[0] + \"\\\" \"+ \" is tagged as \" + \"\\\" \" + tagged_word + \"\\\" \" + \"\\n\")\n",
    "        if(tagged_word in vocab):\n",
    "            result = model.most_similar(tagged_word,topn=15)\n",
    "            for words_score in result:\n",
    "                if(\"_\" not in words_score[0] or words_score[1] < 0.2):\n",
    "                    continue;\n",
    "                wordsTags = words_score[0].split(\"_\")\n",
    "                f.write (\"\\t\"+wordsTags[0] + \"\\t\" + str(words_score[1])+ \"\\t\"+wordsTags[1])\n",
    "                if(tagged_word_pos[1][0:2] in words_score[0]):\n",
    "                    f.write(\"\\t TRUE\\n\")\n",
    "                else:\n",
    "                    f.write(\"\\t FALSE\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.most_similar_cosmul(\"will_MD\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.most_similar_cosmul(\"today_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.most_similar_cosmul(\"than_IN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.most_similar_cosmul(\"guess_VBP\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"guess_VBP\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.most_similar_cosmul(\"day_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.most_similar_cosmul(\"not_RB\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordtag = getPOSTag(\"what does that mean\",\"mean\")\n",
    "print(wordtag)\n",
    "model.most_similar(wordtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No negative sampling\n",
    "model2 = gensim.models.Word2Vec(tokenizedSentences, min_count=20,window=100,size=500,sg=1,iter =15,negative =0)\n",
    "fname = \"LanguageData_deppos_minCount15_skip_iter15_size_500_min_count_50_no_neg\"\n",
    "model2.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"denver_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"will_MD\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"today_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"today_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"than_IN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"than_IN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"guess_VBP\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"guess_VBP\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"day_NN\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.most_similar_cosmul(\"not_RB\",topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [\"not\",\"dont\",\"day\",\"won't\",\"besides\",\"today\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
